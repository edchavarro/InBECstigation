{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InBECstigation-J**\n",
    "\n",
    "An approach for easyly analyze BEC/EAC evidence.\n",
    "\n",
    "- Author: **Eduardo Chavarro Ovalle**\n",
    "    - @echavarro\n",
    "- eduardo.ovalle@kaspersky.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InBECstigation - Approach to analyze BEC cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Business email compromise or email account compromises (BEC/EAC) is one of the threats that represent the biggest number of losses for corporations and individuals, presenting an impact of billions of dollars just based on IC3 stats but considered to be twice this value.\n",
    "While analyzing BEC/EAC cases, we need verify and analyze mailboxes, messages and determine if there are traces of spoofing, vulnerabilities exploitation, unauthorized access to mailboxes or maybe usage of appearance domains while intruding the conversation loop. This means, it is necessary to verify lots of details and here is important to provide tools to improve the analysis tasks and obtain results.\n",
    "This paper focus on the opportunities that python brings to analyze email messages based and how to configure these functions to verify the full set of messages trying to determine key elements to determine the elements affected and the needed controls or additional tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEC/EAC Threat\n",
    "This threat is related to the intrusion to financial and acquisitions communications where adversaries identify clue aspects for acquirement approvals. Adversaries identify the key participants from both sides in a negotiation and determine the moment to involve in the communications, mimicking themselves as the original participant. \n",
    "If the intromission is effective, the real participants won’t identify easily the changes and will continue the information exchange to accomplish all the needed data and procedures for the acquisition.\n",
    "This intrusion is usually product of an initial phishing attack but can also be related to threat actor performing brute force or password guessing, public credentials dump from different services but identifying credentials reusage on multiple services or sometimes vulnerabilities related to mail services exposed over the internet and some cases in the companies’ premises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appearance domains\n",
    "One of the most common trends is the infrastructure acquisition **[ID: T1583]** where the attackers buy domains with a similar literal configuration as the original once. Using this technique, intruders will include themselves in the communication loop spoofing both sides of the communications, to request more information and then modify destinations for financial resources, assets or services. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoofing techniques\n",
    "\n",
    "By exploiting vulnerabilities in software or miss-configurations, threat actors can use non authorized infrastructure to spoof legitimate domains and tamper the source of emails, making the communications seem legit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the algorithm using Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By collecting domains and links in the message body and headers, and using command lines like whois, is it possible to identify fake domains involved in the communications and determine the date when the infrastructure was created, to add this information in the analysis timeline.\n",
    "\n",
    "The first step is to load the evidence in a way that can be parsed and sorted based on real needs for investigation. Dealing with bit OST files or multiple email messages stored in a container it’s a difficult task and the best approach is to load all these information in lists that can be filtered based on metadata, headers and message content. Once information is parsed this way, it will be easy to look for threats or keywords that provide best information in a malleable format.\n",
    "\n",
    "For this purpose, **Pypff** and **Extract_Msg** libraries from python allow to load a file (PST or msg) ang get all the metadata for analysis. Pypff  allows to iterating over all items in the root folder, analyze message by message and extract details for analysis without having to load the PST file in a mail client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypff\n",
    "import whois\n",
    "from email.parser import HeaderParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import extract_msg\n",
    "import glob\n",
    "import time\n",
    "import base64, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:  \n",
    "0. Prepare data\n",
    "1. Appearance domains\n",
    "2. Whois Analysis\n",
    "3. Suspicious Mailboxes\n",
    "4. Messages headers' alerts\n",
    "5. Statistical analysis and timeline\n",
    "6. Subject analysis\n",
    "7. IP addresses analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "**Extract_msg**  automates the extraction of key email data (from, to, cc, date, subject, body) and the email’s attachments so it can be managed as lists and information is stores in text format for its analysis.\n",
    "Once PST files and messages are parsed, it would be necessary to extract relevant information from each message. Next is a list of relevant fields that can be extratcted from messages to perform a fast analysis and identify attack patterns:\n",
    "- x_originating_ip\n",
    "- Message Subject\n",
    "- mailboxes: From, to, CC and return-path\n",
    "- Domains: Extracted from headers or mailboxes.\n",
    "- Authentication-results parameter is suspicious:\n",
    "    - spf=softfail\n",
    "    - dmarc=fail\n",
    "    - compauth=fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_to_df(headers):\n",
    "    return dict([(title.lower(), value) for title, value in headers.items()])\n",
    "\n",
    "def parse_folder(base):\n",
    "    messages = []\n",
    "    for folder in base.sub_folders:\n",
    "        try:\n",
    "            if folder.number_of_sub_folders:\n",
    "                messages += parse_folder(folder)\n",
    "        except:\n",
    "            a=1 \n",
    "        for message in folder.sub_messages:\n",
    "            parser = HeaderParser()\n",
    "            headers = parser.parsestr(message.transport_headers)\n",
    "            messages.append(headers_to_df(headers))\n",
    "    return messages\n",
    "\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    return np.unique(x)\n",
    "\n",
    "def mails(dfmails,m_type):\n",
    "    if m_type != \"from\":\n",
    "        tmp=[]\n",
    "        for line in dfmails:\n",
    "            for m in line.split(','):\n",
    "                tmp.append(m)\n",
    "    else:\n",
    "        tmp=dfmails\n",
    "    tmp2=[]    \n",
    "    for l in unique(tmp):\n",
    "        try:\n",
    "            tmp2.append((l.split(\"<\")[1]).split(\">\")[0])\n",
    "        except:\n",
    "            tmp2.append(l)\n",
    "    return unique(tmp2)\n",
    "\n",
    "def mail_domains(mails_list):\n",
    "    tmp=[]\n",
    "    for l in mails_list:\n",
    "        try:\n",
    "            tmp.append(l.split(\"@\")[1])\n",
    "        except:\n",
    "            print(l, \" not included\")\n",
    "    return unique(tmp)\n",
    "\n",
    "def x_org_ip():\n",
    "    print(\"\",\"* Extracting x_originating_ip\")\n",
    "    x_originating_ip=[]\n",
    "    try:\n",
    "        x_originating_ip=[ip.replace(\"[\",\"\").replace(\"]\",\"\") for ip in df[df[\"x-originating-ip\"].notna()][\"x-originating-ip\"].unique()]\n",
    "        print(\"\\t\",x_originating_ip)\n",
    "    except:\n",
    "        print(\"\\t No x-originating-ip details\")     \n",
    "    return x_originating_ip\n",
    "\n",
    "def mboxes(mbtype,title):\n",
    "    print(\"\",\"- \"+title)\n",
    "    mailboxes=[]\n",
    "    try:\n",
    "        mailboxes=mails(df[df[mbtype].notna()][mbtype].unique(),mbtype)\n",
    "        print(\"\\t\",mailboxes)\n",
    "    except:\n",
    "        print(\"\\t No \",mbtype,\" details\")        \n",
    "    return mailboxes\n",
    "\n",
    "legit_domains=['domain1.com', 'domain2.com']    # YOUR LEGIT DOMAINS HERE\n",
    "tlfile='TL_InBECstigation.log'\n",
    "\n",
    "pstfile=['Exchange.pst']                                      # YOUT PST FILE HERE\n",
    "i=0\n",
    "messages=[]                                                   # Messages will be collected from folder .\\msg yo can change this path at line 84\n",
    "print(\"\",\"Appending Exchange files\")\n",
    "for file in pstfile:\n",
    "    try:\n",
    "        pst = pypff.open(file)\n",
    "    except:\n",
    "        print(\"\",\" -- Error, file \"+file+\" do not exists.\")\n",
    "    root = pst.get_root_folder()\n",
    "    #print(\"\\t\",\"Including container for analysis: \"+file)\n",
    "    messages=messages+parse_folder(root)\n",
    "    pst.close()\n",
    "\n",
    "f = glob.glob('msg/*.msg')\n",
    "\n",
    "for filename in f:\n",
    "    #print(\"\\t\",\"Including msg file for analysis: \"+filename)\n",
    "    msg = extract_msg.Message(filename)\n",
    "    messages.append(headers_to_df(msg.headerDict))\n",
    "\n",
    "df=pd.DataFrame(messages)\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"\\t\",\"Number of messages to analyze: \"+str(len(df.index)))\n",
    "print(\"\\t\",\"Legitimate domains to be evaluated: \")\n",
    "print(\"\\t\",legit_domains)\n",
    "print(\"\\t\",\"Analysis time: \"+current_time)\n",
    "print(\"\",\"********************************************************\")\n",
    "\n",
    "x_originating_ip=x_org_ip()\n",
    "\n",
    "print(\"\\r\\n\",\"* Extracting mailboxes\")\n",
    "mail_from=mboxes(\"from\",\"From Mail\")\n",
    "mail_to=mboxes(\"to\",\"Mail in TO\")\n",
    "mail_cc=mboxes(\"cc\",\"Mail in CC\")\n",
    "return_path=mboxes(\"return-path\", \"Mailboxes in return-path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appearance domains\n",
    "\n",
    "Having a list with mailboxes, it will be easy to extract specific domain (@domain.com) and create a new source for validation.\n",
    "Once all domains are extracted from headers and body, an array can be used to verify each domain against the legitimate once.\n",
    "\n",
    "For this, it will be relevant to include legitimate domains involved in the accessed communication, from all the participating sides. It will be necessary for the analyst to include legitimate domains and pass them to the analysis script. First thing to check can be the *misspelling or similar domains* extracted from the evidence.\n",
    "\n",
    "First thing to check is if the domain extracted from evidence is listed as a real legitimate domain. If don’t, its time to check if it’s similar to a legitimate one. An easy way to verify the similitude based on characters, could be the function **SequenceMatcher** , this function allows to determine how close are a pair of strings resulting in a value between 0 and 1, with 1 as exactly the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance_threshold=0.7\n",
    "susp_strings=[\"spf=softfail\",\"dmarc=fail\",\"compauth=fail\"]\n",
    "\n",
    "def similarities(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def dmn_appearance(dmnfull,real):\n",
    "    dmn=[i for i in dmnfull if i not in real]\n",
    "    dmnappearance=[]\n",
    "    for b in dmn:\n",
    "        for a in real:\n",
    "            if similarities(a, b) > appearance_threshold:\n",
    "                dmnappearance.append(b)\n",
    "    return dmnappearance\n",
    "\n",
    "mbfull=unique(np.concatenate([mail_from,mail_to,mail_cc,return_path]))\n",
    "\n",
    "print(\"\\r\\n\",\"* Extracting domains from pst collection\")\n",
    "domains=mail_domains(mbfull)\n",
    "print(\"\\t\",domains)\n",
    "\n",
    "print(\"\\r\\n\",\"* Verifying possible appearance domains\")  \n",
    "dmn_app=dmn_appearance(domains,legit_domains)\n",
    "print(\"\",dmn_app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on trends to modify just a couple of characters or the TLD (top level domain), having a threshold over 70% (0.7) would be a relevant value to alert the identified domain.\n",
    "\n",
    "## Whois analysis\n",
    "\n",
    "If we identify a domain flagged as a possible appearance domain, it would be important to identify its Creation date and how it matches the timeline for investigation. A good option would be analyzing the **whois** response and verify relevant dates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmn_whois(dmn):\n",
    "    w = whois.whois(dmn)\n",
    "    return w\n",
    "\n",
    "print(\"\\r\\n\",\"* Verifying appearance domains metadata\")\n",
    "\n",
    "susp_rows=[]\n",
    "\n",
    "for dmn in dmn_app:\n",
    "    w=dmn_whois(dmn)\n",
    "    print(\"\\t[suspicious domain] \",dmn)\n",
    "    print(\"\\t\",\"Creation date: \"+str(w.creation_date))\n",
    "    susp_rows.append([str(w.creation_date)+'+00:00',dmn,'Suspicious domain creation time','','','','',''])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the creation date in the suspicious events timeline could present a clue for legitimate domains obtained and used to create the new spoof infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suspicious mailboxes\n",
    "\n",
    "At this point we have a set of possible suspicious mailboxes based on domains they belong. We can create a new list including only suspicious mailboxes to analyze one by one, it would be a match between mailboxes and suspicious domains.\n",
    "\n",
    "Also, we can filter the main data frame to look for fields that contain the specific appearance domain and extract its details in a new list.\n",
    "\n",
    "Having this information, we can extract specific subjects or involved mailboxes to analyze them. It would be interesting to identify what kind of list the mailbox was detected (to, from or CC). This can be used further to try to identify the initial access point to the message loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_messages(msg_field,title,value):\n",
    "    print(\"\",\"Verifying messages \",title)\n",
    "    tmp=pd.DataFrame()\n",
    "    try:\n",
    "        tmp=df[df[msg_field].str.lower().str.contains(value).fillna(False)]\n",
    "        print(\"Subjects: \",unique(tmp[\"subject\"]))\n",
    "    except:\n",
    "        print(\"No \",msg_field, \" details\")\n",
    "    return tmp\n",
    "\n",
    "susp_mb=[]\n",
    "print(\"\\r\\n\",\"* Suspicious mailboxes:\")\n",
    "for m in mbfull:\n",
    "    for d in dmn_app:\n",
    "        try:\n",
    "            if m.index(d): \n",
    "                susp_mb.append(m)\n",
    "        except:\n",
    "            a=1\n",
    "print(\"\",susp_mb)\n",
    "\n",
    "print(\"\\r\\n\",\"* Verifying messages including appearance domains\")\n",
    "\n",
    "susp_msg=pd.DataFrame()\n",
    "for dmn in dmn_app:\n",
    "    tmp=check_messages(\"from\",\"from domain \"+dmn,dmn)\n",
    "    if tmp.size > 0: \n",
    "        susp_msg=pd.concat([susp_msg,tmp]) \n",
    "    tmp=check_messages(\"to\",\"to domain \"+dmn,dmn)\n",
    "    if tmp.size > 0: \n",
    "        susp_msg=pd.concat([susp_msg,tmp]) \n",
    "    tmp=check_messages(\"cc\",\"in CC to domain \"+dmn,dmn)\n",
    "    if tmp.size > 0: \n",
    "        susp_msg=pd.concat([susp_msg,tmp]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages Headers’ alerts\n",
    "\n",
    "Now it’s time to verify the headers and focus on alerts and details. From messages it is possible to verify which of them are being flagged by mail security controls. One easy option could be look for registries where the following strings are triggered: **[\"spf=softfail\",\"dmarc=fail\",\"compauth=fail\"]** (The list can be improved based on analyst experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\r\\n\",\"* Verifying messages where authentication-results parameter is suspicious\")\n",
    "\n",
    "for val in susp_strings:\n",
    "    tmp=check_messages(\"authentication-results\",\"with value \"+val,val)\n",
    "    if tmp.size > 0: \n",
    "        susp_msg=pd.concat([susp_msg,tmp]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis and Timeline\n",
    "\n",
    "While attackers spoof critical mailboxes in the conversation, it doesn’t mean this have been the initial compromised account. In most cases, compromising one of the employers that do not decide or perform the financial or acquisition orders, but is informed about every step in the process, could be the intrusion point. \n",
    "\n",
    "It would be important to perform an analysis where we decipher the legitimate users involved in the initial conversation and register this as part of the general analysis. A type analysis based on TO and CC fields could determine which mailboxes could be included in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if susp_msg.size > 0: \n",
    "    susp_msg[\"date\"]=pd.to_datetime(susp_msg[\"date\"],format='%a, %d %b %Y %X %z')\n",
    "    print(\"\\r\\n\",\"* Suspicious messages: \"+str(len(susp_msg.index)))\n",
    "\n",
    "    for index, item in susp_msg.iterrows():\n",
    "        susp_rows.append([str(item[\"date\"]),item[\"from\"],item[\"return-path\"],item[\"subject\"],item[\"to\"],item[\"thread-index\"],item[\"received-spf\"]])\n",
    "\n",
    "    if len(susp_rows)>1:\n",
    "        print(\"\\r\\n\",\"* Creating timeline for suspicious events to file: \"+tlfile)\n",
    "        sorted_list=sorted(susp_rows, key=lambda x: x[0])\n",
    "        sorted_list.insert(0,[\"date\",\"from\",\"return-path\",\"subject\",\"to\",\"cc\",\"thread-index\",\"received-spf\"])\n",
    "        with open(tlfile, 'w') as file:\n",
    "            for l in sorted_list:\n",
    "                out=str(l)[1:-1]\n",
    "                file.write(out+'\\r\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique([i[3].replace('\\r\\n','') for i in susp_rows]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode strings\n",
    "\n",
    "We can use sentences to get the content from encoded subjects. Just to understand and verify the real objective of the communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeSubject(subject):\n",
    "    encoded_word_regex = r'=\\?{1}(.+)\\?{1}([B|Q])\\?{1}(.+)\\?{1}='\n",
    "    charset, encoding, encoded_text = re.match(encoded_word_regex, subject).groups()\n",
    "    if encoding == 'B':\n",
    "        return base64.b64decode(encoded_text)\n",
    "    else:\n",
    "        return \"unknown ciphering: \" + subject\n",
    "\n",
    "decodeSubject('=?UTF-8?B?RG8gWW91IERvIEFueSBvZiBUaGVzZSBFbWJhcnJhc3NpbmcgVGhpbmdzPw==?=')    #Your encoded UTF strings here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject analysis\n",
    "Identified the Subject and topics selected by threat actors to jump into the coversation can add other details related to attackments and communicattions attempts to different areas that could haven't been detected during the primari analysis. Using the messages lists we can extract all the messages related for aditional analysis and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unique(susp_rows[\"subject\"]))\n",
    "pattern='.*Your partnership is required urgently'               #Your interesting subject here you may use regular expressions\n",
    "mask=df['subject'].str.contains(pattern,case=False,na=False)\n",
    "print('Messages related to pattern '+pattern+': '+str(len(df[mask])))\n",
    "print('Exporting to file mysubject.csv')\n",
    "tmp=df.loc[mask, ['date','from','to','subject']]\n",
    "tmp[\"date\"]=pd.to_datetime(tmp[\"date\"],format='%a, %d %b %Y %X %z')\n",
    "tmp.to_csv('mysubject.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP addresses analysis\n",
    "We can use intelligence platforms to have a quick look to domains and IP addresses' reputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "apikey = 'YOUR API KEY'  #### ENTER API KEY HERE ####\n",
    "\n",
    "requests.urllib3.disable_warnings()\n",
    "client = requests.session()\n",
    "client.verify = False\n",
    "domainErrors = []\n",
    "delay = {}\n",
    "\n",
    "def DomainScanner(domain): #'From Matthew Clairmont VT_Domain_Scanner\n",
    "    url = 'https://www.virustotal.com/vtapi/v2/url/scan'\n",
    "    params = {'apikey': apikey, 'url': domain}\n",
    "\n",
    "    try:\n",
    "        r = client.post(url, params=params)\n",
    "    except requests.ConnectTimeout as timeout:\n",
    "        print('-','Connection timed out. Error is as follows-'+str(timeout))\n",
    "\n",
    "    print('',domain)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            jsonResponse = r.json()\n",
    "            if jsonResponse['response_code'] != 1:\n",
    "                print('\\t','There was an error submitting the domain for scanning.')\n",
    "            elif jsonResponse['response_code'] == -2:\n",
    "                print('\\t',str(domain)+' is queued for scanning.')\n",
    "                delay[domain] = 'queued'\n",
    "            else:\n",
    "                print('\\t',str(domain)+' was scanned successfully.')\n",
    "\n",
    "        except ValueError:\n",
    "            print('\\t','There was an error when scanning '+str(domain))\n",
    "\n",
    "        time.sleep(1)  ############### IF YOU HAVE A PRIVATE ACCESS YOU CAN CHANGE THIS TO 1 ###################\n",
    "        return delay\n",
    "\n",
    "    elif r.status_code == 204:\n",
    "        print('','Received HTTP 204 response. You may have exceeded your API request quota or rate limit.')\n",
    "\n",
    "def DomainReportReader(domain, delay): #'From Matthew Clairmont VT_Domain_Scanner\n",
    "    if delay:\n",
    "        if domain in delay:\n",
    "            time.sleep(10)\n",
    "\n",
    "    url = 'https://www.virustotal.com/vtapi/v2/url/report'\n",
    "    params = {'apikey': apikey, 'resource': domain}\n",
    "\n",
    "    try:\n",
    "        r = client.post(url, params=params)\n",
    "    except requests.ConnectTimeout as timeout:\n",
    "        print('\\t','Connection timed out. Error '+str(timeout))\n",
    "        exit(1)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            jsonResponse = r.json()\n",
    "            if jsonResponse['response_code'] == 0:\n",
    "                print('\\t','There was an error submitting the domain for scanning.')\n",
    "                pass\n",
    "\n",
    "            scandate = jsonResponse['scan_date']\n",
    "            positives = jsonResponse['positives']\n",
    "            total = jsonResponse['total']\n",
    "\n",
    "            data = [scandate, domain, positives, total]\n",
    "            return data\n",
    "\n",
    "        except ValueError:\n",
    "            print('\\t','There was an error when scanning '+str(domain))\n",
    "\n",
    "        except KeyError:\n",
    "            print('\\t','There was an error when scanning '+str(domain))\n",
    "\n",
    "    elif r.status_code == 204:\n",
    "        print('','Received HTTP 204 response. You may have exceeded your API request quota or rate limit.')\n",
    "        time.sleep(10)\n",
    "        DomainReportReader(domain, delay)\n",
    "\n",
    "\n",
    "print('','* Verifying IP addresses against VirusTotal')\n",
    "for ip in x_originating_ip:\n",
    "    try:\n",
    "        delay = DomainScanner(ip)\n",
    "        data = DomainReportReader(ip, delay)\n",
    "        if data:\n",
    "            print('\\t',data)\n",
    "            time.sleep(1)  \n",
    "    except Exception as err:  \n",
    "        print('','- Encountered an error but scanning will continue.'+str(err))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
